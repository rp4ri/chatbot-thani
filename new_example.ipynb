{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "training, output, all_words, tags = load_training_data()\n",
    "\n",
    "# Definir modelo\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = ChatbotModel(len(training[0]), 128, len(output[0]), 0.5)\n",
    "\n",
    "# Entrenar modelo\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    inputs = torch.from_numpy(training).float()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = criterion(outputs, output)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Guardar modelo en un archivo pkl\n",
    "with open('models/chatbot_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m message\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     20\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m response \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39;49mget_response(message)\n\u001b[0;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mChatbot:\u001b[39m\u001b[39m'\u001b[39m, response)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\chatbot.py:70\u001b[0m, in \u001b[0;36mChatbot.get_response\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     69\u001b[0m     \u001b[39m# Obtener respuesta de acuerdo a la etiqueta\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_tag(sentence)\n\u001b[0;32m     71\u001b[0m     \u001b[39mfor\u001b[39;00m intent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintents[\u001b[39m'\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     72\u001b[0m         \u001b[39mif\u001b[39;00m intent[\u001b[39m'\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m tag:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\chatbot.py:61\u001b[0m, in \u001b[0;36mChatbot.predict_tag\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_tag\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     60\u001b[0m     \u001b[39m# Obtener predicción de etiqueta\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m     bow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbag_of_words(sentence)\n\u001b[0;32m     62\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(bow)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     63\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\chatbot.py:52\u001b[0m, in \u001b[0;36mChatbot.bag_of_words\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbag_of_words\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m---> 52\u001b[0m     sentence_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclean_up_sentence(sentence)\n\u001b[0;32m     53\u001b[0m     bag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_words), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     54\u001b[0m     \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_words):\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\chatbot.py:45\u001b[0m, in \u001b[0;36mChatbot.clean_up_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_up_sentence\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     43\u001b[0m     \u001b[39m# Tokenización y eliminación de stopwords\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     sentence_words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence)\n\u001b[1;32m---> 45\u001b[0m     sentence_words \u001b[39m=\u001b[39m remove_stopwords(sentence_words)\n\u001b[0;32m     46\u001b[0m     \u001b[39m# Stemming\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     stemmer \u001b[39m=\u001b[39m SnowballStemmer(\u001b[39m'\u001b[39m\u001b[39mspanish\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\stopword_remover.py:15\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m stopwords_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39mspanish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Remove stopwords from tokens\u001b[39;00m\n\u001b[0;32m     18\u001b[0m filtered_tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords_list]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text):\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.chatbot import Chatbot\n",
    "\n",
    "# Cargar modelo desde archivo pkl\n",
    "chatbot = Chatbot('models/chatbot_model.pkl')\n",
    "\n",
    "# Obtener respuesta del chatbot\n",
    "while True:\n",
    "    message = input('You: ')\n",
    "    if message.lower() == 'exit':\n",
    "        break\n",
    "    response = chatbot.get_response(message)\n",
    "    print('Chatbot:', response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 83 (4035665283.py, line 85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 85\u001b[1;36m\u001b[0m\n\u001b[1;33m    input_bag = preprocess(input_text, words)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 83\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "import random\n",
    "\n",
    "training, output, all_words, tags = load_training_data()\n",
    "\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self, model_path):\n",
    "        with open(model_path, 'rb') as file:\n",
    "            self.model = pickle.load(file)\n",
    "        \n",
    "        with open('data/intents.json', encoding='iso-8859-1') as file:\n",
    "            self.intents = json.load(file)\n",
    "        \n",
    "        self.all_words = all_words\n",
    "        self.tags = tags\n",
    "    \n",
    "    def clean_up_sentence(self, sentence):\n",
    "        # Tokenización y eliminación de stopwords\n",
    "        sentence_words = nltk.word_tokenize(sentence)\n",
    "        sentence_words = remove_stopwords(sentence_words)\n",
    "        # Stemming\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "        return sentence_words\n",
    "\n",
    "    def bag_of_words(self, sentence):\n",
    "        sentence_words = self.clean_up_sentence(sentence)\n",
    "        bag = np.zeros(len(self.all_words), dtype=np.float32)\n",
    "        for idx, word in enumerate(self.all_words):\n",
    "            if word in sentence_words:\n",
    "                bag[idx] = 1\n",
    "        return bag\n",
    "\n",
    "    def predict_tag(self, sentence):\n",
    "        # Obtener predicción de etiqueta\n",
    "        bow = self.bag_of_words(sentence)\n",
    "        x = torch.from_numpy(bow).unsqueeze(0)\n",
    "        output = self.model(x)\n",
    "        _, predicted = torch.max(output, dim=1)\n",
    "        tag = self.tags[predicted.item()]\n",
    "        return tag\n",
    "\n",
    "    def preprocess(input_text, words):\n",
    "        # Tokenize the input text\n",
    "        tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "        # Stem the tokens\n",
    "        stemmer = SnowballStemmer('spanish')\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "        # Create a bag of words vector\n",
    "        input_bag = torch.zeros(len(words))\n",
    "        for word in stemmed_tokens:\n",
    "            if word in words:\n",
    "                input_bag[words.index(word)] = 1\n",
    "\n",
    "        return input_bag\n",
    "    def get_response(self, sentence):\n",
    "        # Preprocess the input text\n",
    "    input_bag = preprocess(input_text, words)\n",
    "    #input_bag = preprocess_input(input_text)\n",
    "    #print(input_bag)\n",
    "\n",
    "    # Convert input to a PyTorch tensor\n",
    "    #input_tensor = torch.tensor(input_bag).float().unsqueeze(0)\n",
    "    #input_tensor = torch.tensor(input_bag, dtype=torch.float).clone().detach().unsqueeze(0)\n",
    "    input_tensor = input_bag.clone().detach().unsqueeze(0)\n",
    "\n",
    "    # Use the model to predict the output\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Convert the output to a numpy array\n",
    "    output_np = output.detach().numpy()\n",
    "\n",
    "    # Find the tag with the highest probability\n",
    "    tag_index = np.argmax(output_np)\n",
    "\n",
    "    if output_np[0][tag_index] < 0.5:\n",
    "        return \"Lo siento, no te he entendido. ¿Podrías reformular la pregunta?\"\n",
    "\n",
    "    tag = tags[tag_index]\n",
    "\n",
    "    # Choose a random response from the tag's list of responses\n",
    "    responses = []\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            responses = intent['responses']\n",
    "\n",
    "    return random.choice(responses)\n",
    "        # Obtener respuesta de acuerdo a la etiqueta\n",
    "        tag = self.predict_tag(sentence)\n",
    "        for intent in self.intents['intents']:\n",
    "            if intent['tag'] == tag:\n",
    "                responses = intent['responses']\n",
    "        return np.random.choice(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Load the model from the file\n",
    "with open('models/chatbot_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Load the words and tags from the data file\n",
    "with open('data/intents.json', encoding='iso-8859-1') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "words = []\n",
    "tags = []\n",
    "patterns = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    \n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize and stem the words in the pattern\n",
    "        tokens = nltk.word_tokenize(pattern.lower())\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        words.extend(stemmed_tokens)\n",
    "        patterns.append((stemmed_tokens, tag))\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "tags = sorted(list(set(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mYou: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     response \u001b[39m=\u001b[39m chatbot\u001b[39m.\u001b[39;49mget_response(user_input)\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mChatbot:\u001b[39m\u001b[39m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[16], line 68\u001b[0m, in \u001b[0;36mChatbot.get_response\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_response\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     67\u001b[0m     \u001b[39m# Obtener respuesta de acuerdo a la etiqueta\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_tag(sentence)\n\u001b[0;32m     69\u001b[0m     \u001b[39mfor\u001b[39;00m intent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintents[\u001b[39m'\u001b[39m\u001b[39mintents\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m     70\u001b[0m         \u001b[39mif\u001b[39;00m intent[\u001b[39m'\u001b[39m\u001b[39mtag\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m tag:\n",
      "Cell \u001b[1;32mIn[16], line 59\u001b[0m, in \u001b[0;36mChatbot.predict_tag\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_tag\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     58\u001b[0m     \u001b[39m# Obtener predicción de etiqueta\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m     bow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbag_of_words(sentence)\n\u001b[0;32m     60\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(bow)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     61\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(x)\n",
      "Cell \u001b[1;32mIn[16], line 50\u001b[0m, in \u001b[0;36mChatbot.bag_of_words\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbag_of_words\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[1;32m---> 50\u001b[0m     sentence_words \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclean_up_sentence(sentence)\n\u001b[0;32m     51\u001b[0m     bag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_words), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     52\u001b[0m     \u001b[39mfor\u001b[39;00m idx, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_words):\n",
      "Cell \u001b[1;32mIn[16], line 43\u001b[0m, in \u001b[0;36mChatbot.clean_up_sentence\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_up_sentence\u001b[39m(\u001b[39mself\u001b[39m, sentence):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# Tokenización y eliminación de stopwords\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     sentence_words \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mword_tokenize(sentence)\n\u001b[1;32m---> 43\u001b[0m     sentence_words \u001b[39m=\u001b[39m remove_stopwords(sentence_words)\n\u001b[0;32m     44\u001b[0m     \u001b[39m# Stemming\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     stemmer \u001b[39m=\u001b[39m SnowballStemmer(\u001b[39m'\u001b[39m\u001b[39mspanish\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\src\\stopword_remover.py:15\u001b[0m, in \u001b[0;36mremove_stopwords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     12\u001b[0m stopwords_list \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39mspanish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Remove stopwords from tokens\u001b[39;00m\n\u001b[0;32m     18\u001b[0m filtered_tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mlower() \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stopwords_list]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1281\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1278\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1341\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\n\u001b[0;32m   1333\u001b[0m     \u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m, realign_boundaries: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1334\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mstr\u001b[39m]:\n\u001b[0;32m   1335\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1336\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1338\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1339\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1340\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1341\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1329\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1327\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1328\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1329\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1330\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1459\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1447\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1448\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1457\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1458\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1459\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1460\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1461\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:321\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    319\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    320\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 321\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    322\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1431\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[\u001b[39mslice\u001b[39m]:\n\u001b[0;32m   1430\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1431\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_potential_end_contexts(text):\n\u001b[0;32m   1432\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1433\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m previous_slice \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[0;32m   1394\u001b[0m previous_match \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1395\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text):\n\u001b[0;32m   1396\u001b[0m \n\u001b[0;32m   1397\u001b[0m     \u001b[39m# Get the slice of the previous word\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m     before_text \u001b[39m=\u001b[39m text[previous_slice\u001b[39m.\u001b[39mstop : match\u001b[39m.\u001b[39mstart()]\n\u001b[0;32m   1399\u001b[0m     index_after_last_space \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_last_whitespace_index(before_text)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "model_path = 'models/chatbot_model.pkl'\n",
    "chatbot = Chatbot(model_path)\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    response = chatbot.get_response(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text, words):\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Create a bag of words vector\n",
    "    input_bag = torch.zeros(len(words))\n",
    "    for word in stemmed_tokens:\n",
    "        if word in words:\n",
    "            input_bag[words.index(word)] = 1\n",
    "\n",
    "    return input_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_response(input_text):\n",
    "    # Preprocess the input text\n",
    "    input_bag = preprocess(input_text, words)\n",
    "    #input_bag = preprocess_input(input_text)\n",
    "    #print(input_bag)\n",
    "\n",
    "    # Convert input to a PyTorch tensor\n",
    "    #input_tensor = torch.tensor(input_bag).float().unsqueeze(0)\n",
    "    #input_tensor = torch.tensor(input_bag, dtype=torch.float).clone().detach().unsqueeze(0)\n",
    "    input_tensor = input_bag.clone().detach().unsqueeze(0)\n",
    "\n",
    "    # Use the model to predict the output\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Convert the output to a numpy array\n",
    "    output_np = output.detach().numpy()\n",
    "\n",
    "    # Find the tag with the highest probability\n",
    "    tag_index = np.argmax(output_np)\n",
    "\n",
    "    if output_np[0][tag_index] < 0.5:\n",
    "        return \"Lo siento, no te he entendido. ¿Podrías reformular la pregunta?\"\n",
    "\n",
    "    tag = tags[tag_index]\n",
    "\n",
    "    # Choose a random response from the tag's list of responses\n",
    "    responses = []\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            responses = intent['responses']\n",
    "\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x108 and 31x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_response(\u001b[39m'\u001b[39;49m\u001b[39mHola\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     12\u001b[0m input_tensor \u001b[39m=\u001b[39m input_bag\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Use the model to predict the output\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m output \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m     17\u001b[0m \u001b[39m# Convert the output to a numpy array\u001b[39;00m\n\u001b[0;32m     18\u001b[0m output_np \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mChatbotModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     25\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     26\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x108 and 31x128)"
     ]
    }
   ],
   "source": [
    "generate_response('Hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "training, output, all_words, tags = load_training_data()\n",
    "\n",
    "# Definir modelo\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = ChatbotModel(len(training[0]), 128, len(output[0]), 0.5)\n",
    "\n",
    "# Entrenar modelo\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    inputs = torch.from_numpy(training).float()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = criterion(outputs, output)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Guardar modelo en un archivo pkl\n",
    "with open('models/chatbot_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hol']\n",
      "torch.Size([31])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'¡Que tengas un buen día!'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "#training, output, all_words, tags = load_training_data()\n",
    "training, output, words, tags = load_training_data()\n",
    "\n",
    "def preprocess(input_text, words):\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    print(stemmed_tokens)\n",
    "\n",
    "    # Create a bag of words vector\n",
    "    input_bag = torch.zeros(len(words))\n",
    "    for word in stemmed_tokens:\n",
    "        if word in words:\n",
    "            input_bag[words.index(word)] = 1\n",
    "\n",
    "    return input_bag\n",
    "def generate_response(input_text):\n",
    "    # Preprocess the input text\n",
    "    input_bag = preprocess(input_text, words)\n",
    "    #input_bag = preprocess_input(input_text)\n",
    "    print(input_bag.shape)\n",
    "\n",
    "    # Convert input to a PyTorch tensor\n",
    "    input_tensor = input_bag.clone().detach().unsqueeze(0)\n",
    "\n",
    "    # Use the model to predict the output\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Convert the output to a numpy array\n",
    "    output_np = output.detach().numpy()\n",
    "\n",
    "    # Find the tag with the highest probability\n",
    "    tag_index = np.argmax(output_np)\n",
    "\n",
    "    if output_np[0][tag_index] < 0.5:\n",
    "        return \"Lo siento, no te he entendido. ¿Podrías reformular la pregunta?\"\n",
    "\n",
    "    tag = tags[tag_index]\n",
    "\n",
    "    # Choose a random response from the tag's list of responses\n",
    "    responses = []\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            responses = intent['responses']\n",
    "\n",
    "    return random.choice(responses)\n",
    "generate_response('Hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Load the model from the file\n",
    "with open('models/chatbot_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "# Load the words and tags from the data file\n",
    "with open('data/intents.json', encoding='iso-8859-1') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "words = []\n",
    "tags = []\n",
    "patterns = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    tag = intent['tag']\n",
    "    tags.append(tag)\n",
    "    \n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize and stem the words in the pattern\n",
    "        tokens = nltk.word_tokenize(pattern.lower())\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        words.extend(stemmed_tokens)\n",
    "        patterns.append((stemmed_tokens, tag))\n",
    "\n",
    "words = sorted(list(set(words)))\n",
    "tags = sorted(list(set(tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(input_text, words):\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    # Create a bag of words vector\n",
    "    input_bag = torch.zeros(len(words))\n",
    "    for word in stemmed_tokens:\n",
    "        if word in words:\n",
    "            input_bag[words.index(word)] = 1\n",
    "\n",
    "    return input_bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text):\n",
    "    # Preprocess the input text\n",
    "    input_bag = preprocess(input_text, words)\n",
    "    #input_bag = preprocess_input(input_text)\n",
    "    #print(input_bag)\n",
    "\n",
    "    # Convert input to a PyTorch tensor\n",
    "    #input_tensor = torch.tensor(input_bag).float().unsqueeze(0)\n",
    "    #input_tensor = torch.tensor(input_bag, dtype=torch.float).clone().detach().unsqueeze(0)\n",
    "    input_tensor = input_bag.clone().detach().unsqueeze(0)\n",
    "\n",
    "    # Use the model to predict the output\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Convert the output to a numpy array\n",
    "    output_np = output.detach().numpy()\n",
    "\n",
    "    # Find the tag with the highest probability\n",
    "    tag_index = np.argmax(output_np)\n",
    "\n",
    "    if output_np[0][tag_index] < 0.5:\n",
    "        return \"Lo siento, no te he entendido. ¿Podrías reformular la pregunta?\"\n",
    "\n",
    "    tag = tags[tag_index]\n",
    "\n",
    "    # Choose a random response from the tag's list of responses\n",
    "    responses = []\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            responses = intent['responses']\n",
    "\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x108 and 31x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m generate_response(\u001b[39m'\u001b[39;49m\u001b[39mHola\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[30], line 13\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(input_text)\u001b[0m\n\u001b[0;32m     10\u001b[0m input_tensor \u001b[39m=\u001b[39m input_bag\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Use the model to predict the output\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m output \u001b[39m=\u001b[39m model(input_tensor)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Convert the output to a numpy array\u001b[39;00m\n\u001b[0;32m     16\u001b[0m output_np \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[27], line 24\u001b[0m, in \u001b[0;36mChatbotModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x)\n\u001b[0;32m     25\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     26\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rodri\\piton\\chatbot-thani\\chatbot-thani\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x108 and 31x128)"
     ]
    }
   ],
   "source": [
    "generate_response('Hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "training, output, all_words, tags, data = load_training_data()\n",
    "\n",
    "# Definir modelo\n",
    "class ChatbotModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "#model = ChatbotModel(len(training[0]), 128, len(output[0]), 0.5)\n",
    "model = ChatbotModel(len(training[0]), 128, len(output[0]), 0.5)\n",
    "\n",
    "# Entrenar modelo\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(100):\n",
    "    inputs = torch.from_numpy(training).float()\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = criterion(outputs, output)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Guardar modelo en un archivo pkl\n",
    "with open('models/chatbot_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'adios',\n",
       " 'agradezc',\n",
       " 'avanz',\n",
       " 'ayud',\n",
       " 'ayudart',\n",
       " 'bienven',\n",
       " 'buen',\n",
       " 'carrer',\n",
       " 'com',\n",
       " 'con',\n",
       " 'conmig',\n",
       " 'consej',\n",
       " 'consult',\n",
       " 'curriculum',\n",
       " 'de',\n",
       " 'deb',\n",
       " 'decision',\n",
       " 'deprim',\n",
       " 'dia',\n",
       " 'dias',\n",
       " 'dorm',\n",
       " 'el',\n",
       " 'en',\n",
       " 'encontr',\n",
       " 'entiend',\n",
       " 'entrev',\n",
       " 'es',\n",
       " 'escuch',\n",
       " 'estas',\n",
       " 'esto',\n",
       " 'estoy',\n",
       " 'estres',\n",
       " 'graci',\n",
       " 'gust',\n",
       " 'ha',\n",
       " 'habl',\n",
       " 'hac',\n",
       " 'hast',\n",
       " 'hol',\n",
       " 'interpersonal',\n",
       " 'jef',\n",
       " 'la',\n",
       " 'las',\n",
       " 'line',\n",
       " 'lueg',\n",
       " 'mal',\n",
       " 'me',\n",
       " 'mejor',\n",
       " 'mi',\n",
       " 'mis',\n",
       " 'much',\n",
       " 'muy',\n",
       " 'nadi',\n",
       " 'necesit',\n",
       " 'no',\n",
       " 'noch',\n",
       " 'nos',\n",
       " 'par',\n",
       " 'personal',\n",
       " 'piens',\n",
       " 'por',\n",
       " 'prepar',\n",
       " 'problem',\n",
       " 'pront',\n",
       " 'proxim',\n",
       " 'pued',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'recomiend',\n",
       " 'relacion',\n",
       " 'ruptur',\n",
       " 'se',\n",
       " 'sent',\n",
       " 'sid',\n",
       " 'sient',\n",
       " 'situacion',\n",
       " 'super',\n",
       " 'tal',\n",
       " 'tard',\n",
       " 'te',\n",
       " 'ten',\n",
       " 'teng',\n",
       " 'tom',\n",
       " 'trabaj',\n",
       " 'trat',\n",
       " 'trist',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'una',\n",
       " 'util',\n",
       " 'valor',\n",
       " 'vem',\n",
       " 'vid',\n",
       " '¡cha',\n",
       " '¡hast',\n",
       " '¡hol',\n",
       " '¡qu',\n",
       " '¿com',\n",
       " '¿cual',\n",
       " '¿en',\n",
       " '¿necesit',\n",
       " '¿pued',\n",
       " '¿qu']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agradecimiento',\n",
       " 'consejo',\n",
       " 'despedida',\n",
       " 'problema_personal',\n",
       " 'problema_profesional',\n",
       " 'saludo']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hol']\n",
      "torch.Size([108])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hola, bienvenido a mi consulta en línea. ¿En qué puedo ayudarte?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "\n",
    "from src.stopword_remover import remove_stopwords\n",
    "\n",
    "from src.data_loader import load_training_data\n",
    "\n",
    "#training, output, all_words, tags = load_training_data()\n",
    "training, output, words, tags, data = load_training_data()\n",
    "\n",
    "def preprocess(input_text, words):\n",
    "    # Tokenize the input text\n",
    "    tokens = nltk.word_tokenize(input_text.lower())\n",
    "\n",
    "    # Stem the tokens\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    print(stemmed_tokens)\n",
    "\n",
    "    # Create a bag of words vector\n",
    "    input_bag = torch.zeros(len(words))\n",
    "    for word in stemmed_tokens:\n",
    "        if word in words:\n",
    "            input_bag[words.index(word)] = 1\n",
    "\n",
    "    return input_bag\n",
    "def generate_response(input_text):\n",
    "    # Preprocess the input text\n",
    "    input_bag = preprocess(input_text, words)\n",
    "    #input_bag = preprocess_input(input_text)\n",
    "    print(input_bag.shape)\n",
    "\n",
    "    # Convert input to a PyTorch tensor\n",
    "    input_tensor = input_bag.clone().detach().unsqueeze(0)\n",
    "\n",
    "    # Use the model to predict the output\n",
    "    output = model(input_tensor)\n",
    "\n",
    "    # Convert the output to a numpy array\n",
    "    output_np = output.detach().numpy()\n",
    "\n",
    "    # Find the tag with the highest probability\n",
    "    tag_index = np.argmax(output_np)\n",
    "\n",
    "    if output_np[0][tag_index] < 0.5:\n",
    "        return \"Lo siento, no te he entendido. ¿Podrías reformular la pregunta?\"\n",
    "\n",
    "    tag = tags[tag_index]\n",
    "\n",
    "    # Choose a random response from the tag's list of responses\n",
    "    responses = []\n",
    "    for intent in data['intents']:\n",
    "        if intent['tag'] == tag:\n",
    "            responses = intent['responses']\n",
    "\n",
    "    return random.choice(responses)\n",
    "generate_response('hola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([48, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " '!',\n",
       " ',',\n",
       " '.',\n",
       " '?',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '¡',\n",
       " '¿']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agradecimiento',\n",
       " 'consejo',\n",
       " 'despedida',\n",
       " 'problema_personal',\n",
       " 'problema_profesional',\n",
       " 'saludo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-thani",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
